{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8807970779778823, 0.8807970779778823, 0.8807970779778823],\n",
       " [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]],\n",
       " 0.6931471805599453,\n",
       " [-0.8807970779778823, -0.8807970779778823],\n",
       " [0.5, 0.5],\n",
       " [2.0, 2.0, 2.0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize():\n",
    "    x = [1., -1.]\n",
    "    w = [[1.,1.,1.], [-1.,-1.,-1.]]\n",
    "    b = [0.,0.,0.]\n",
    "    k = [0.,0.,0.]\n",
    "    h = [0.,0.,0.]\n",
    "    v = [[1.,1.],[-1.,-1.],[-1.,-1.]]\n",
    "    c = [0., 0.]\n",
    "    t = [1., 0.]\n",
    "    s = [0., 0.]\n",
    "    y = []\n",
    "    l = 0\n",
    "    return x,w,b,k,h,v,c,t,s,y,l\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + math.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    sum_exp = 0\n",
    "    for i in x:\n",
    "        sum_exp += math.exp(i)\n",
    "        return math.exp(i)/ sum_exp\n",
    "\n",
    "def forward_pass(x,w,b,k,h,v,c,t,s,y,loss):\n",
    "    #Forward pass:\n",
    "    for lin in range(3): # k nodes len(k)\n",
    "        for i in range(2): #input nodes\n",
    "            k[lin] += w[i][lin] * x[i]\n",
    "        k[lin] += b[lin]\n",
    "    #print(k)\n",
    "\n",
    "    for hid in range(3):\n",
    "        h[hid] = sigmoid(k[hid]) # 1. / (1. + math.exp(-k[hid])) \n",
    "    #print(h)\n",
    "\n",
    "    for soft in range(len(s)):\n",
    "        for hi in range(len(h)):\n",
    "            s[soft] += v[hi][soft] * h[hi] #hidden * weights\n",
    "        s[soft] += c[soft] #second bias\n",
    "    #print(s)\n",
    "\n",
    "    sum_exp = 0\n",
    "    for i in s:\n",
    "        sum_exp += math.exp(i)\n",
    "\n",
    "    for i in range(len(s)):  \n",
    "        y.append(math.exp(s[i])/ sum_exp)\n",
    "    #print(y)\n",
    "\n",
    "    for node in range(len(y)):\n",
    "        if t[node] == 1.: l = (- math.log(y[node]))\n",
    "\n",
    "    return x,w,b,k,h,v,c,t,s,y,l\n",
    "    #correct output (t), the loss (l), and all intermediate values (return k, h, o(s from softmax), y, l) \n",
    "\n",
    "x,w,b,k,h,v,c,t,s,y,l = initialize()\n",
    "x,w,b,k,h,v,c,t,s,y,l = forward_pass(x,w,b,k,h,v,c,t,s,y,l)\n",
    "h, v, l, s, y, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "def backward_pass(x,w,b,k,h,v,s,y):\n",
    "    dk = k\n",
    "    dh = h\n",
    "    dv = v\n",
    "    dw = w\n",
    "    db = b\n",
    "    dy = y\n",
    "    \n",
    "    for elem in range(len(y)):\n",
    "        if elem != 1:\n",
    "            dy[elem] = y[elem] - 1\n",
    "    print(dy)\n",
    "\n",
    "    for soft in range(len(s)):\n",
    "        for hi in range(len(h)): #sigmoid hidden layer\n",
    "            dv[hi][soft] = dy[soft] * h[hi]\n",
    "\n",
    "    for soft in range(len(s)):\n",
    "        for hi in range(len(h)): #sigmoid hidden layer\n",
    "            dh[hi] = dy[soft] * v[hi][soft]\n",
    "\n",
    "    dc = dy\n",
    "\n",
    "    for lin in range(len(dk)):     \n",
    "        dk[lin] = dh[lin] * h[lin] * (1-h[lin])\n",
    "\n",
    "\n",
    "    for lin in range(len(k)):\n",
    "        for inp in range(len(x)):\n",
    "            dw[inp][lin] = dk[lin] * x[inp]     \n",
    "    \n",
    "    db[lin] = dk[lin] \n",
    "\n",
    "    return dw, db, dv, dc, dy\n",
    "\n",
    "dw, db, dv, dc, dy = backward_pass(x,w,b,k,h,v,s,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synth(num_train=60_000, num_val=10_000, seed=0):\n",
    "    \"\"\"\n",
    "    Load some very basic synthetic data that should be easy to classify. Two features, so that we can plot the\n",
    "    decision boundary (which is an ellipse in the feature space).\n",
    "    :param num_train: Number of training instances\n",
    "    :param num_val: Number of test/validation instances\n",
    "    :param num_features: Number of features per instance\n",
    "    :return: Two tuples and an integer: (xtrain, ytrain), (xval, yval), num_cls. The first contains a matrix of training\n",
    "     data with 2 features as a numpy floating point array, and the corresponding classification labels as a numpy\n",
    "     integer array. The second contains the test/validation data in the same format. The last integer contains the\n",
    "     number of classes (this is always 2 for this function).\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    THRESHOLD = 0.6\n",
    "    quad = np.asarray([[1, -0.05], [1, .4]])\n",
    "\n",
    "    ntotal = num_train + num_val\n",
    "\n",
    "    x = np.random.randn(ntotal, 2)\n",
    "\n",
    "    # compute the quadratic form\n",
    "    q = np.einsum('bf, fk, bk -> b', x, quad, x)\n",
    "    y = (q > THRESHOLD).astype(np.int)\n",
    "\n",
    "    return (x[:num_train, :], y[:num_train]), (x[num_train:, :], y[num_train:]), 2\n",
    "\n",
    "def load_mnist(final=False, flatten=True):\n",
    "    \"\"\"\n",
    "    Load the MNIST data.\n",
    "    :param final: If true, return the canonical test/train split. If false, split some validation data from the training\n",
    "       data and keep the test data hidden.\n",
    "    :param flatten: If true, each instance is flattened into a vector, so that the data is returns as a matrix with 768\n",
    "        columns. If false, the data is returned as a 3-tensor preserving each image as a matrix.\n",
    "    :return: Two tuples and an integer: (xtrain, ytrain), (xval, yval), num_cls. The first contains a matrix of training\n",
    "     data and the corresponding classification labels as a numpy integer array. The second contains the test/validation\n",
    "     data in the same format. The last integer contains the number of classes (this is always 2 for this function).\n",
    "     \"\"\"\n",
    "\n",
    "    if not os.path.isfile('mnist.pkl'):\n",
    "        init()\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load()\n",
    "    xtl, xsl = xtrain.shape[0], xtest.shape[0]\n",
    "\n",
    "    if flatten:\n",
    "        xtrain = xtrain.reshape(xtl, -1)\n",
    "        xtest  = xtest.reshape(xsl, -1)\n",
    "\n",
    "    if not final: # return the flattened images\n",
    "        return (xtrain[:-5000], ytrain[:-5000]), (xtrain[-5000:], ytrain[-5000:]), 10\n",
    "\n",
    "    return (xtrain, ytrain), (xtest, ytest), 10\n",
    "\n",
    "# Numpy-only MNIST loader. Courtesy of Hyeonseok Jung\n",
    "# https://github.com/hsjeong5/MNIST-for-Numpy\n",
    "\n",
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xval, yval), num_cls = load_synth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a training loop for your network and show that the training loss drops as training progresses.\n",
    "Some tips:\n",
    "- How you initialize the weights is an important choice. For now, you can set the regular weights to some normally distributed random value, and the bias weights to 0. We'll delve into this question more in later lectures.\n",
    "- You can use the python random package to generate normally distributed random values, or generate some values online and hardcode them.\n",
    "- Our data loaders provide the target classes as integer values. It's easiest to work out the derivative in terms of these values directly, but you can also convert them to one-hot vectors as shown in the image.\n",
    "- You can use stochastic gradient descent, calculating the loss over one instance at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random():\n",
    "    w = np.random.randn(2,3)# [[1.,1.,1.], [-1.,-1.,-1.]]\n",
    "    v = np.random.randn(3,2) # [[1.,1.],[-1.,-1.],[-1.,-1.]]\n",
    "\n",
    "    b = [0.,0.,0.]\n",
    "    c = [0., 0.]\n",
    "\n",
    "    k = [0.,0.,0.]\n",
    "    h = [0.,0.,0.]\n",
    "    s = [0., 0.]\n",
    "    y = []\n",
    "    l = 0\n",
    "    \n",
    "    return w,b,k,h,v,c,s,y,l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-84d366c80929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# plt.plot(i,l, 'o')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mst_grad_des\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-84d366c80929>\u001b[0m in \u001b[0;36mst_grad_des\u001b[0;34m(alpha, xtrain, ytrain)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for each input x and target t in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;31m# dw, db, dv, dc, dy = backward_pass(x,w,b,k,h,v,s,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m# w = w - alpha * dw #derivative of loss of x and t in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-eeee3766896b>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(x, w, b, k, h, v, c, t, s, y, loss)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "def st_grad_des(alpha, xtrain, ytrain):\n",
    "    loss = []\n",
    "    w,b,k,h,v,c,s,y,l = initialize_random()    \n",
    "    for i in range(1):\n",
    "        for x, t in zip(xtrain, ytrain): # for each input x and target t in data\n",
    "            x,w,b,k,h,v,c,t,s,y,l = forward_pass(x,w,b,k,h,v,c,t,s,y,l)\n",
    "            # dw, db, dv, dc, dy = backward_pass(x,w,b,k,h,v,s,y)\n",
    "            # w = w - alpha * dw #derivative of loss of x and t in data\n",
    "            # b = b - alpha * db\n",
    "            # v = v - alpha * dv\n",
    "            # c = c - alpha * dc\n",
    "            # y = y - alpha * dy\n",
    "        # plt.plot(i,l, 'o')\n",
    "\n",
    "st_grad_des(1,xtrain, ytrain)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4feba3f04a4b90fbb0a806c309de7b3295a27177045ae635acaab27ba8cd4be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
